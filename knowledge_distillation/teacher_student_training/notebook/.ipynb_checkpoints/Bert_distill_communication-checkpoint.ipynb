{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# PyTorch Pretrained BERT on Communication Slot Tagging\n",
    "\n",
    "In this notebook we will show how to perform slot tagging on the Teams dataset. Follow the requirements to run Azure ML notebook by checking https://github.com/danielsc/dogbreeds/blob/master/dog-breed-simple.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haochu\\AppData\\Local\\Continuum\\anaconda3\\envs\\azure\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.33\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Workspace and select gpu cluster\n",
    "\n",
    "if there is not existing cluster, create one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library configuration succeeded\n",
      "https://ms.portal.azure.com/#@microsoft.onmicrosoft.com/resource/subscriptions/ddb33dc4-889c-4fa1-90ce-482d793d6480/resourceGroups/DevExp/providers/Microsoft.MachineLearningServices/workspaces/DevExperimentation\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "#subscription_id = \"4a66f470-dd54-4c5e-bd19-8cb65a426003\"\n",
    "#resource_group  = \"AML_Playground\"\n",
    "#workspace_name  = \"Teams_ws\"\n",
    "\n",
    "subscription_id = \"ddb33dc4-889c-4fa1-90ce-482d793d6480\"\n",
    "resource_group = \"DevExp\"\n",
    "workspace_name = \"DevExperimentation\"\n",
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "    ws.write_config()\n",
    "    print('Library configuration succeeded')\n",
    "    print('https://ms.portal.azure.com/#@microsoft.onmicrosoft.com/resource' + ws.get_details()['id'])\n",
    "except:\n",
    "    print('Workspace not found')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "#cluster_name = \"p100cluster\"\n",
    "cluster_name  =\"P100-SingleGPU\"\n",
    "\n",
    "try:\n",
    "    compute_target = ws.compute_targets[cluster_name]\n",
    "    print('Found existing compute target.')\n",
    "except KeyError:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC6s_v2', \n",
    "                                                           idle_seconds_before_scaledown=1800,\n",
    "                                                           min_nodes=0, \n",
    "                                                           max_nodes=10)\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Datastore and upload local data\n",
    "\n",
    "When you have large data and model, you need to create one seperate Datastore.\n",
    "\n",
    "If not, AML will have error and you can't track your outputs. \n",
    "\n",
    "Each workspace is associated with a default Azure Blob datastore named 'workspaceblobstore'. In this work, we use this default datastore to store our local data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = ws.get_default_datastore()\n",
    "from azureml.core import Workspace,Datastore \n",
    "\n",
    "ds = Datastore.get(ws, datastore_name='compliant_lu_haochu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for bert_data/uncased_model/outputs_base_uncased\\model_config.json\n",
      "Target already exists. Skipping upload for bert_data/uncased_model/outputs_base_uncased\\vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$AZUREML_DATAREFERENCE_ee226e320acf4235a4bd20b88c26c50b\n"
     ]
    }
   ],
   "source": [
    "#upload local model\n",
    "model_path_on_datastore = f'bert_data/uncased_model/outputs_base_uncased' #cased model,vocab is too small? Do not have frequent word like common\n",
    "ds_model = ds.path(model_path_on_datastore)\n",
    "ds.upload(src_dir=r'D:\\dl_repo\\Data_model\\Communication_teacher_uncased',\n",
    "          target_path= model_path_on_datastore,\n",
    "          overwrite=False,\n",
    "          show_progress=True)\n",
    "print(ds_model.as_mount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for Communication_slot_model_unsupervised\\bert_config.json\n",
      "Target already exists. Skipping upload for Communication_slot_model_unsupervised\\pytorch_model.bin\n",
      "Target already exists. Skipping upload for Communication_slot_model_unsupervised\\vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$AZUREML_DATAREFERENCE_a2c043d92b044c608e6838f898d79d6b\n"
     ]
    }
   ],
   "source": [
    "#upload unsupervised local model\n",
    "model_path_on_datastore = 'Communication_slot_model_unsupervised' #cased model,vocab is too small? Do not have frequent word like common\n",
    "ds_model_unsupervised = ds.path(model_path_on_datastore)\n",
    "ds.upload(src_dir=r'D:\\dl_repo\\Data_model\\Communication_unsupervised',\n",
    "          target_path= model_path_on_datastore,\n",
    "          overwrite=False,\n",
    "          show_progress=True)\n",
    "print(ds_model_unsupervised.as_mount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for datasets/Teams_communication\\BIO-tags.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\comm_train_prod.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\Target_set_message_new_conll.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\test.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\test_blind_old.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\test_nonormal.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\train.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\train_19k_only.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\train_legacy_1m.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\train_positive_generated.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\train_teams.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\train_valid_oct.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\valid.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\valid_aug.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\valid_sep.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\valid_valid.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading D:\\dl_repo\\Data_model\\Communication_data\\valid_dec.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for datasets/Teams_communication\\generated_data\\communication_message_generated_contact.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\generated_data\\communication_message_generated_no_contact.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\raequery_for_prod_with_target.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery_contact_message_generated.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery_expanision-outputs.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery_for_prod.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded D:\\dl_repo\\Data_model\\Communication_data\\valid_dec.txt, 1 files out of an estimated total of 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery_for_prod_from_valid_nov.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery_for_prod_from_valid_nov_normalized.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery_for_prod_with_civ.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery_from_civ.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery_merged.txt\n",
      "Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery_no_contact_message_generated.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_ce2df08c41ad48f88adb44a0853988ae"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upload local data set\n",
    "path_on_datastore = 'datasets/Teams_communication'\n",
    "ds_data_communication = ds.path(path_on_datastore)\n",
    "ds.upload(src_dir=r'D:\\dl_repo\\Data_model\\Communication_data',\n",
    "          target_path= path_on_datastore,\n",
    "          overwrite=False,\n",
    "          show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for Communication_slot_model_fine-tuned\\bert_config.json\n",
      "Target already exists. Skipping upload for Communication_slot_model_fine-tuned\\eval_results.txt\n",
      "Target already exists. Skipping upload for Communication_slot_model_fine-tuned\\model_config.json\n",
      "Target already exists. Skipping upload for Communication_slot_model_fine-tuned\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$AZUREML_DATAREFERENCE_79ad4d425a894461b86e6e1b05f81c42\n"
     ]
    }
   ],
   "source": [
    "#upload pre-trained model\n",
    "#TODO: reuse azure blob path in aml\n",
    "model_path_on_datastore = 'Communication_slot_model_fine-tuned' #cased model,vocab is too small? Do not have frequent word like common\n",
    "ds_model_pretrained = ds.path(model_path_on_datastore)\n",
    "ds.upload(src_dir=r'D:\\dl_repo\\Data_model\\Communication_finetuned',\n",
    "          target_path= model_path_on_datastore,\n",
    "          overwrite=False,\n",
    "          show_progress=True)\n",
    "print(ds_model.as_mount())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an experiment\n",
    "Create an Experiment to track all the runs in your workspace. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'UDA' \n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit your Job\n",
    "The follow section creates one pytorch estimator, you can easily specify your parameters.\n",
    "\n",
    "When you submit your job, it will autoamtically upload your local repo to the cloud cluster. \n",
    "\n",
    "You can also submit tensorflow or keras job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Finetuning\n",
    "\n",
    "With pre-trained model and labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BATCH AI\n",
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "\n",
    "\n",
    "script_params = {\n",
    "    #'--data_dir': ds_data.as_mount(),\n",
    "    #'--data_dir': 'teams_data', #update for golden data\n",
    "    '--data_dir': ds_data_communication.as_mount(), #update for communication data\n",
    "    #'--train_dir':ds.path(f'datasets/Teams_communication/train_valid_oct.txt').as_mount(),\n",
    "    #'--train_dir':ds.path(f'datasets/Teams_communication/train_teams.txt').as_mount(),\n",
    "    #'--train_dir':ds.path(f'datasets/Teams_communication/comm_train_prod.txt').as_mount(),\n",
    "    '--train_dir':ds.path(f'datasets/Communication_prod_data/communication_slot_train.txt').as_mount(),\n",
    "    '--valid_dir':ds.path(f'datasets/Teams_communication/valid_dec.txt').as_mount(),\n",
    "    '--test_generated_dir':ds.path(f'datasets/Teams_communication/generated_data/communication_message_generated_no_contact.txt').as_mount(),\n",
    "    '--test_generated_no_contact_dir':ds.path(f'datasets/Teams_communication/generated_data/communication_message_generated_contact.txt').as_mount(),\n",
    "    '--target_set_dir':ds.path(f'datasets/Teams_communication/Target_set_message_new_conll.txt').as_mount(),\n",
    "    \n",
    "    #'--teacher_model_path':ds_model_pretrained.as_mount(),\n",
    "    #'--teacher_model_path':ds.path(f'bert_data/outputs').as_mount(),\n",
    "    #'--teacher_model_path':ds.path(f'bert_data/outputs1').as_mount(),#from prod labeled sources\n",
    "     #'--bert_model': ds_model.as_mount(),\n",
    "    \n",
    "    \n",
    "    ##for uncased longer teacher\n",
    "    '--teacher_model_path':ds.path(f'bert_data/uncased_model/outputs_base_uncased_no_basic_tokenizer').as_mount(),#from prod labeled sources\n",
    "    '--bert_model':'bert-base-uncased', \n",
    "    '--do_lower_case':'',\n",
    "    '--do_basic_tokenize':'', # add for comparation\n",
    "    \n",
    "    \n",
    "    '--task_name':'ner',\n",
    "    '--output_dir':'./outputs',\n",
    "    '--do_train':'',\n",
    "    '--do_eval':'',\n",
    "    #'--crf':'',\n",
    "    #'--learning_rate':'0.001',#add large learning rate for student model\n",
    "    '--learning_rate':'1e-4',#smaller learning rate given init\n",
    "    '--num_train_epochs':'50', ##get larger number when you have smaller unsupervised data\n",
    "    '--warmup_proportion':'0.1',\n",
    "    #'--max_seq_length':'32',\n",
    "    '--max_seq_length':'128',\n",
    "    '--train_batch_size':'64',\n",
    "    \"--temperature\": '1',\n",
    "    \"--alpha\": '1',#for ablation study,weight for labeled data\n",
    "    \"--beta\": '1', #for ablation study,weight for unlabeled data\n",
    "    #\"--unsupervised_train_corpus\":'./lm_data/train.txt' #add for debug\n",
    "    #\"--unsupervised_train_corpus\":'./lm_data/augmented_data.txt' #add for debug\n",
    "    '--unsupervised_train_corpus':ds.path(f'datasets/Teams_communication/raw_generated_data/rawquery_merged.txt').as_mount()\n",
    "    #'--unsupervised_train_corpus':ds.path(f'datasets/Teams_communication/raw_generated_data/rawquery_for_prod.txt').as_mount()\n",
    "    #'--unsupervised_train_corpus':ds.path(f'datasets/Teams_communication/raw_generated_data/rawquery_for_prod_from_valid_nov_normalized.txt').as_mount()   \n",
    "    #'--unsupervised_train_corpus':ds.path(f'datasets/Teams_communication/raw_generated_data/rawquery_from_civ.txt').as_mount()\n",
    "    #'--unsupervised_train_corpus':ds.path(f'datasets/Teams_communication/raw_generated_data/rawquery_expanision-outputs.txt').as_mount()\n",
    "    #'--unsupervised_train_corpus':ds.path(f'datasets/Teams_communication/raw_generated_data/raequery_for_prod_with_target.txt').as_mount()\n",
    "    #'--unsupervised_train_corpus':ds.path(f'datasets/Teams_communication/raw_generated_data/rawquery_for_prod_with_civ.txt').as_mount()\n",
    "    #\"--unsupervised_train_corpus\":'./lm_data/communication_postive_generated_raw.txt' #add for debug\n",
    "    \n",
    "    \n",
    "    #'--multi_gpu':'',\n",
    "}\n",
    "\n",
    "estimator10 = PyTorch(source_directory='..', \n",
    "                    script_params=script_params,\n",
    "                    compute_target=compute_target, \n",
    "                    entry_script='src/distillation.py',\n",
    "                    #pip_packages=['pandas','pytorch-pretrained-bert==0.4.0','seqeval==0.0.5'],\n",
    "                    #pip_packages=['pandas','pytorch-pretrained-bert==0.6.1','seqeval==0.0.5','nltk'],\n",
    "                    pip_packages=['pandas','pytorch-pretrained-bert==0.6.1','seqeval==0.0.5','transformers==2.1.1','nltk'],\n",
    "                    use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up your running environment\n",
    "\n",
    "You can create your own virtual environment. It takes a while the first time you submit the job. If you do not change dependency, the job submission will be fast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcr.microsoft.com/azureml/base-gpu:intelmpi2018.3-cuda9.0-cudnn7-ubuntu16.04\n"
     ]
    }
   ],
   "source": [
    "print(estimator10.run_config.environment.docker.base_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Conda environment specification. The dependencies defined in this file will\r\n",
      "# be automatically provisioned for runs with userManagedDependencies=False.\r\n",
      "\n",
      "# Details about the Conda environment file format:\r\n",
      "# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\r\n",
      "\n",
      "name: project_environment\n",
      "dependencies:\n",
      "  # The python interpreter version.\r\n",
      "  # Currently Azure ML only supports 3.5.2 and later.\r\n",
      "- python=3.6.2\n",
      "\n",
      "- pip:\n",
      "  - pandas\n",
      "  - pytorch-pretrained-bert==0.6.1\n",
      "  - seqeval==0.0.5\n",
      "  - transformers==2.1.1\n",
      "  - nltk\n",
      "  - azureml-defaults\n",
      "  - torch==1.0\n",
      "  - torchvision==0.2.1\n",
      "  - horovod==0.15.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(estimator10.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit and Monitor your run\n",
    "\n",
    "You can also find your previous runs if your open the azure portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(estimator10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140c8c7e54b4406a9d5aa7a706cbfc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.hyperdrive import *\n",
    "import math\n",
    "ps = RandomParameterSampling(\n",
    "    {\n",
    "        '--learning_rate': loguniform(math.log(1e-5), math.log(1e-3)),\n",
    "        #'--beta': choice(1,0),\n",
    "        #'--alpha':choice(1,0),\n",
    "        #\"--temperature\":choice(1,5,10,20)\n",
    "        \"--encoder_type\": choice('LSTM','GRU'),\n",
    "        \"--hidden_units\": choice(300,400,500,600),\n",
    "        \"--weight_decay\": choice (0.0,0.001,0.01)\n",
    "    }\n",
    ")\n",
    "\n",
    "policy = BanditPolicy(evaluation_interval=2, slack_factor=0.2)\n",
    "\n",
    "\n",
    "hdc = HyperDriveConfig(estimator=estimator10, \n",
    "                          hyperparameter_sampling=ps, \n",
    "                          policy=policy, \n",
    "                          primary_metric_name='best_val_f1', \n",
    "                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                          max_total_runs=20,\n",
    "                          max_concurrent_runs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The same input parameter(s) are specified in estimator/run_config script params and HyperDrive parameter space. HyperDrive parameter space definition will override these duplicate entries. ['--learning_rate'] is the list of overridden parameter(s).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef59785d57347b29665d0636242846b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hd_run = experiment.submit(hdc)\n",
    "RunDetails(hd_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training with unsupervised in domain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BATCH AI\n",
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "#script_params = {\n",
    "\n",
    "#    '--data_path': 'data/sst2',\n",
    "#    '--num_epochs': '10',\n",
    "#    '--embedding_type': 'elmo',\n",
    "#    '--output_dir': './outputs'\n",
    "#}\n",
    "\n",
    "\n",
    "script_params = {\n",
    "\n",
    "    #'--train_corpus' : './lm_data/train.txt',\n",
    "    '--train_corpus' : ds.path(f'azureml-blobstore-1dd9ddd7-6d52-40ad-8b00-be73cd90fd63/lm_data_merged_4_percent.txt').as_mount(),\n",
    "    #'--data_dir': 'gold_teams_data', #update for golden data\n",
    "    '--bert_model': ds_model.as_mount(),\n",
    "    '--output_dir':ds.path(f'pretrained-model/output_merged_unlabeled_4_percent').as_mount(),\n",
    "    '--do_train':'',\n",
    "    '--learning_rate':'5e-5',#larger learning rate given init\n",
    "    '--num_train_epochs':'2',\n",
    "   # '--warmup_proportion':'0.1',\n",
    "    '--max_seq_length':'32',\n",
    "    '--train_batch_size':'64'\n",
    "    #'--train_batch_size':'16' #smaller batch for low resource\n",
    "    \n",
    "    #'--multi_gpu':'',\n",
    "}\n",
    "\n",
    "estimator10 = PyTorch(source_directory='..', \n",
    "                    script_params=script_params,\n",
    "                    compute_target=compute_target, \n",
    "                    entry_script='simple_lm_finetune.py',\n",
    "                    #pip_packages=['allennlp','pandas','tensorflow','pytorch-pretrained-bert'],\n",
    "                    #pip_packages=['pandas','pytorch-pretrained-bert==0.4.0','seqeval==0.0.5'],\n",
    "                    pip_packages=['pandas','pytorch-pretrained-bert==0.6.1','seqeval==0.0.5'],\n",
    "                    use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and register the best model\n",
    "Once all the runs complete, we can find the run that produced the model with the highest evaluation f1.\n",
    "\n",
    "ref: https://github.com/microsoft/AzureML-BERT/blob/master/finetune/PyTorch/notebooks/Pretrained-BERT-NER.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "print(best_run)\n",
    "print('Best Run is:\\n  F1: {0:.5f} \\n  Learning rate: {1:.8f}'.format(\n",
    "        best_run_metrics['best_val_f1'][-1],\n",
    "        best_run_metrics['lr']\n",
    "     ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "sample_data = ds.path(f'datasets/Transfer_set/testing_sample_input.txt').as_mount()\n",
    "print (sample_data)\n",
    "file = open(sample_data)\n",
    "for line in file:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# To do\n",
    "\n",
    "Fine-Tuning BERT with Hyperparameter Tuning\n",
    "\n",
    "Fine-Tuning BERT with mutiple GPU\n",
    "\n",
    "Generate azure pipeline\n",
    "\n",
    "\n",
    "Link to the blob and datastore account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore\n",
    "# Default datastore \n",
    "def_data_store = ws.get_default_datastore()\n",
    "\n",
    "\n",
    "# The following call GETS the Azure Blob Store associated with your workspace.\n",
    "# Note that workspaceblobstore is **the name of this store and CANNOT BE CHANGED and must be used as is** \n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))\n",
    "\n",
    "# Get file storage associated with the workspace\n",
    "#it will be put into files share folder in the workspace?\n",
    "def_file_store = Datastore(ws, \"workspacefilestore\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_data_store.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_blob_store.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_file_store.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to specific  blob\n",
    "\n",
    "we need to upload data into specific blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload local model\n",
    "model_path_on_datastore = 'Teams_slot_model' #cased model,vocab is too small? Do not have frequent word like common\n",
    "ds_model = ds.path(model_path_on_datastore)\n",
    "def_file_store.upload(src_dir=r'D:\\dl_repo\\Tagging_data\\bert-base-English-cased-pytorch',\n",
    "          target_path= model_path_on_datastore,\n",
    "          overwrite=False,\n",
    "          show_progress=True)\n",
    "print(ds_model.as_mount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "# Define intermediate data using PipelineData\n",
    "# Syntax\n",
    "\n",
    "# PipelineData(name, \n",
    "#              datastore=None, \n",
    "#              output_name=None, \n",
    "#              output_mode='mount', \n",
    "#              output_path_on_compute=None, \n",
    "#              output_overwrite=None, \n",
    "#              data_type=None, \n",
    "#              is_directory=None)\n",
    "\n",
    "\n",
    "output_dir = PipelineData(\"results\",datastore=def_file_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register new datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Register the datastore with the workspace\n",
    "ds = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                             datastore_name='BERT_Preprocessed_Data',\n",
    "                                             container_name='data',\n",
    "                                             account_name='<name goes here>', \n",
    "                                             account_key='<key goes here>'\n",
    "                                            )\n",
    "\n",
    "# Help from: https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-access-data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
