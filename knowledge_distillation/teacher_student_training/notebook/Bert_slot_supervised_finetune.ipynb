{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# PyTorch Pretrained BERT on Teams Slot Tagging\n",
    "\n",
    "In this notebook we will show how to perform slot tagging on the Teams dataset. Follow the requirements to run Azure ML notebook by checking https://github.com/danielsc/dogbreeds/blob/master/dog-breed-simple.ipynb"
   ]
  },
  {
   "source": [
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    ""
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "SDK version: 1.0.33\n"
    }
   ],
   "metadata": {},
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Workspace and select gpu cluster\n",
    "\n",
    "if there is not existing cluster, create one"
   ]
  },
  {
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "subscription_id = \"4a66f470-dd54-4c5e-bd19-8cb65a426003\"\n",
    "resource_group  = \"AML_Playground\"\n",
    "workspace_name  = \"Teams_ws\"\n",
    "\n",
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "    ws.write_config()\n",
    "    print('Library configuration succeeded')\n",
    "    print('https://ms.portal.azure.com/#@microsoft.onmicrosoft.com/resource' + ws.get_details()['id'])\n",
    "except:\n",
    "    print('Workspace not found')\n",
    "\n",
    ""
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Library configuration succeeded\nhttps://ms.portal.azure.com/#@microsoft.onmicrosoft.com/resource/subscriptions/4a66f470-dd54-4c5e-bd19-8cb65a426003/resourceGroups/AML_Playground/providers/Microsoft.MachineLearningServices/workspaces/Teams_ws\n"
    }
   ],
   "metadata": {},
   "execution_count": 48
  },
  {
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "cluster_name = \"p100cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ws.compute_targets[cluster_name]\n",
    "    print('Found existing compute target.')\n",
    "except KeyError:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC6s_v2', \n",
    "                                                           idle_seconds_before_scaledown=1800,\n",
    "                                                           min_nodes=0, \n",
    "                                                           max_nodes=10)\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found existing compute target.\n"
    }
   ],
   "metadata": {},
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Datastore and upload local data\n",
    "\n",
    "When you have large data and model, you need to create one seperate Datastore.\n",
    "\n",
    "If not, AML will have error and you can't track your outputs. \n",
    "\n",
    "Each workspace is associated with a default Azure Blob datastore named 'workspaceblobstore'. In this work, we use this default datastore to store our local data."
   ]
  },
  {
   "source": [
    "ds = ws.get_default_datastore()"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "#upload local model\n",
    "model_path_on_datastore = 'Teams_slot_model' #cased model,vocab is too small? Do not have frequent word like common\n",
    "ds_model = ds.path(model_path_on_datastore)\n",
    "ds.upload(src_dir=r'D:\\dl_repo\\Tagging_data\\bert-base-English-cased-pytorch',\n",
    "          target_path= model_path_on_datastore,\n",
    "          overwrite=False,\n",
    "          show_progress=True)\n",
    "print(ds_model.as_mount())"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING - Target already exists. Skipping upload for Teams_slot_model\\bert_config.json\nWARNING - Target already exists. Skipping upload for Teams_slot_model\\pytorch_model.bin\nWARNING - Target already exists. Skipping upload for Teams_slot_model\\vocab.txt\n$AZUREML_DATAREFERENCE_ca7e0d0fdc984f7daff28ed53f474f3a\n"
    }
   ],
   "metadata": {},
   "execution_count": 51
  },
  {
   "source": [
    "#upload unsupervised local model\n",
    "model_path_on_datastore = 'Communication_slot_model_unsupervised' #cased model,vocab is too small? Do not have frequent word like common\n",
    "ds_model_unsupervised = ds.path(model_path_on_datastore)\n",
    "ds.upload(src_dir=r'D:\\dl_repo\\Data_model\\Communication_unsupervised',\n",
    "          target_path= model_path_on_datastore,\n",
    "          overwrite=False,\n",
    "          show_progress=True)\n",
    "print(ds_model_unsupervised.as_mount())"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING - Target already exists. Skipping upload for Communication_slot_model_unsupervised\\bert_config.json\nWARNING - Target already exists. Skipping upload for Communication_slot_model_unsupervised\\pytorch_model.bin\nWARNING - Target already exists. Skipping upload for Communication_slot_model_unsupervised\\vocab.txt\n$AZUREML_DATAREFERENCE_948406753abe4dae96bb9fd471cfc00e\n"
    }
   ],
   "metadata": {},
   "execution_count": 52
  },
  {
   "source": [
    "#upload local data set\n",
    "path_on_datastore = 'datasets/Teams_communication'\n",
    "ds_data_communication = ds.path(path_on_datastore)\n",
    "ds.upload(src_dir=r'D:\\dl_repo\\Data_model\\Communication_data',\n",
    "          target_path= path_on_datastore,\n",
    "          overwrite=False,\n",
    "          show_progress=True)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING - Target already exists. Skipping upload for datasets/Teams_communication\\BIO-tags.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\comm_train_prod.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\test.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\test_blind_old.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\test_nonormal.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\train.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\train_19k_only.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\train_legacy_1m.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\train_positive_generated.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\train_teams.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\train_valid_oct.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\valid.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\valid_aug.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\valid_sep.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\valid_valid.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\generated_data\\communication_message_generated_contact.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\generated_data\\communication_message_generated_no_contact.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery_contact_message_generated.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery_expanision-outputs.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery_for_prod.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery_for_prod_from_valid_nov.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery_for_prod_from_valid_nov_normalized.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery_from_civ.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery_merged.txt\nWARNING - Target already exists. Skipping upload for datasets/Teams_communication\\raw_generated_data\\rawquery_no_contact_message_generated.txt\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "$AZUREML_DATAREFERENCE_996a0d3f55db4f77b6bce5e449edcf85"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {},
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an experiment\n",
    "Create an Experiment to track all the runs in your workspace. \n"
   ]
  },
  {
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'Teams_slot_uncased' \n",
    "experiment = Experiment(ws, name=experiment_name)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit your Job\n",
    "The follow section creates one pytorch estimator, you can easily specify your parameters.\n",
    "\n",
    "When you submit your job, it will autoamtically upload your local repo to the cloud cluster. \n",
    "\n",
    "You can also submit tensorflow or keras job"
   ]
  },
  {
   "source": [
    "##BATCH AI\n",
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "script_params = {\n",
    "    #'--data_dir': ds_data.as_mount(),\n",
    "    #'--data_dir': 'Communication_data', #update for golden data\n",
    "    '--data_dir': ds_data_communication.as_mount(), #update for golden data\n",
    "    '--train_dir':ds.path(f'datasets/Teams_communication/comm_train_prod.txt').as_mount(),\n",
    "    #'--train_dir':ds.path(f'datasets/Teams_communication/train_teams.txt').as_mount(),\n",
    "    '--eval_dir':ds.path(f'datasets/Teams_communication/valid.txt').as_mount(),\n",
    "    '--test_dir':ds.path(f'datasets/Teams_communication/test.txt').as_mount(),\n",
    "    \n",
    "    #'--bert_model':ds_model_unsupervised.as_mount(),#for pre-trained model with indomain unsupervised data\n",
    "    #'--bert_model':ds.path(f'pretrained-model/output_merged_unlabeled').as_mount(),\n",
    "    #'--bert_model':ds.path(f'pretrained-model/output_merged_unlabeled_4_percent').as_mount(),\n",
    "    #'--bert_model':'bert-large-uncased',#for pre-trained model\n",
    "    \n",
    "    #for pre-trained model\n",
    "    #'--bert_model':'bert-base-uncased',\n",
    "    '--bert_model':'bert-base-uncased',\n",
    "    '--do_lower_case':'',\n",
    "    \n",
    "    \n",
    "    '--task_name':'ner',\n",
    "    '--output_dir':'./outputs',\n",
    "    #'--output_dir':ds.path(f'bert_data/uncased_model/outputs_base_uncased_real').as_mount(),\n",
    "    '--do_train':'',\n",
    "    '--do_eval':'',\n",
    "    '--learning_rate':'2e-5',#larger learning rate given init\n",
    "    #'--learning_rate':'5e-5',#larger learning rate given init\n",
    "    '--num_train_epochs':'20',\n",
    "    '--warmup_proportion':'0.1',\n",
    "    #'--max_seq_length':'32',\n",
    "    '--max_seq_length':'128',\n",
    "    #'--train_batch_size':'8'#8 for smaller base\n",
    "    '--train_batch_size':'64'#8 for smaller base\n",
    "}\n",
    "\n",
    "estimator10 = PyTorch(source_directory='..', \n",
    "                    script_params=script_params,\n",
    "                    compute_target=compute_target, \n",
    "                    entry_script='Teacher_training.py',\n",
    "                    #pip_packages=['pandas','pytorch-pretrained-bert==0.4.0','seqeval==0.0.5'],\n",
    "                    pip_packages=['pandas','pytorch-pretrained-bert==0.6.1','seqeval==0.0.5','transformers==2.1.1'],\n",
    "                    use_gpu=True)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up your running environment\n",
    "\n",
    "You can create your own virtual environment. It takes a while the first time you submit the job. If you do not change dependency, the job submission will be fast.\n"
   ]
  },
  {
   "source": [
    "print(estimator10.run_config.environment.docker.base_image)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "mcr.microsoft.com/azureml/base-gpu:intelmpi2018.3-cuda9.0-cudnn7-ubuntu16.04\n"
    }
   ],
   "metadata": {},
   "execution_count": 56
  },
  {
   "source": [
    "print(estimator10.conda_dependencies.serialize_to_string())"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "# Conda environment specification. The dependencies defined in this file will\n# be automatically provisioned for runs with userManagedDependencies=False.\n\n# Details about the Conda environment file format:\n# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\n\nname: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n  - pandas\n  - pytorch-pretrained-bert==0.6.1\n  - seqeval==0.0.5\n  - transformers==2.1.1\n  - azureml-defaults\n  - torch==1.0\n  - torchvision==0.2.1\n  - horovod==0.15.2\n\n"
    }
   ],
   "metadata": {},
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit and Monitor your run\n",
    "\n",
    "You can also find your previous runs if your open the azure portal"
   ]
  },
  {
   "source": [
    "run = experiment.submit(estimator10)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 58
  },
  {
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6723d255fd7b4c70bdc9409a7595d0c4"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {},
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter selection"
   ]
  },
  {
   "source": [
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.hyperdrive import *\n",
    "import math\n",
    "ps = RandomParameterSampling(\n",
    "    {\n",
    "        #'--learning_rate': loguniform(math.log(1e-5), math.log(1e-4)),\n",
    "        '--learning_rate': choice(1e-5,2e-5,5e-5,1e-4),\n",
    "        #'--beta': choice(1,5,10,20)\n",
    "        #\"--temperature\":choice(1,5,10,20)\n",
    "        \"--train_batch_size\": choice(8,16,32,64)\n",
    "    }\n",
    ")\n",
    "\n",
    "policy = BanditPolicy(evaluation_interval=2, slack_factor=0.2)\n",
    "\n",
    "\n",
    "hdc = HyperDriveConfig(estimator=estimator10, \n",
    "                          hyperparameter_sampling=ps, \n",
    "                          policy=policy, \n",
    "                          primary_metric_name='best_val_f1', \n",
    "                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                          max_total_runs=10,\n",
    "                          max_concurrent_runs=4)\n",
    ""
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 60
  },
  {
   "source": [
    "hd_run = experiment.submit(hdc)\n",
    "RunDetails(hd_run).show()"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "104a3c70d8b84829b67bccb5abff0b89"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {},
   "execution_count": 61
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# To do\n",
    "\n",
    "Fine-Tuning BERT with Hyperparameter Tuning\n",
    "\n",
    "Fine-Tuning BERT with mutiple GPU\n",
    "\n",
    "Generate azure pipeline\n",
    "\n",
    "\n",
    "Link to the blob and datastore account"
   ]
  },
  {
   "source": [
    "from azureml.core import Workspace, Datastore\n",
    "# Default datastore \n",
    "def_data_store = ws.get_default_datastore()\n",
    "\n",
    "\n",
    "# The following call GETS the Azure Blob Store associated with your workspace.\n",
    "# Note that workspaceblobstore is **the name of this store and CANNOT BE CHANGED and must be used as is** \n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))\n",
    "\n",
    "# Get file storage associated with the workspace\n",
    "#it will be put into files share folder in the workspace?\n",
    "def_file_store = Datastore(ws, \"workspacefilestore\")\n",
    "\n",
    "\n",
    ""
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Blobstore's name: workspaceblobstore\n"
    }
   ],
   "metadata": {},
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "def_data_store.name"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'workspaceblobstore'"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {},
   "execution_count": 63
  },
  {
   "source": [
    "def_blob_store.name"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'workspaceblobstore'"
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {},
   "execution_count": 64
  },
  {
   "source": [
    "def_file_store.name"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'workspacefilestore'"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {},
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to specific  blob\n",
    "\n",
    "we need to upload data into specific blob"
   ]
  },
  {
   "source": [
    "#upload local model\n",
    "model_path_on_datastore = f'bert_data/uncased_model/outputs_base_uncased_no_basic_tokenizer' #cased model,vocab is too small? Do not have frequent word like common\n",
    "ds_model = ds.path(model_path_on_datastore)\n",
    "ds.upload(src_dir=r'D:\\dl_repo\\BERT-NER\\comm_out',\n",
    "          target_path= model_path_on_datastore,\n",
    "          overwrite=False,\n",
    "          show_progress=True)\n",
    "print(ds_model.as_mount())"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING - Target already exists. Skipping upload for bert_data/uncased_model/outputs_base_uncased_no_basic_tokenizer\\bert_config.json\nWARNING - Target already exists. Skipping upload for bert_data/uncased_model/outputs_base_uncased_no_basic_tokenizer\\model_config.json\nWARNING - Target already exists. Skipping upload for bert_data/uncased_model/outputs_base_uncased_no_basic_tokenizer\\pytorch_model.bin\nWARNING - Target already exists. Skipping upload for bert_data/uncased_model/outputs_base_uncased_no_basic_tokenizer\\pytorch_model_basic_tokenizer.bin\nWARNING - Target already exists. Skipping upload for bert_data/uncased_model/outputs_base_uncased_no_basic_tokenizer\\student_model.bin\nWARNING - Target already exists. Skipping upload for bert_data/uncased_model/outputs_base_uncased_no_basic_tokenizer\\student_model_no_crf.bin\n$AZUREML_DATAREFERENCE_6d2d43d7dffe4e218b12917640a82203\n"
    }
   ],
   "metadata": {},
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "# Define intermediate data using PipelineData\n",
    "# Syntax\n",
    "\n",
    "# PipelineData(name, \n",
    "#              datastore=None, \n",
    "#              output_name=None, \n",
    "#              output_mode='mount', \n",
    "#              output_path_on_compute=None, \n",
    "#              output_overwrite=None, \n",
    "#              data_type=None, \n",
    "#              is_directory=None)\n",
    "\n",
    "\n",
    "output_dir = PipelineData(\"results\",datastore=def_file_store)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 67
  },
  {
   "source": [
    "output_dir"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "$AZUREML_DATAREFERENCE_results"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "metadata": {},
   "execution_count": 68
  },
  {
   "source": [
    "ds_model"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "$AZUREML_DATAREFERENCE_bf753cf9267f4b169251a6bfc753a941"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "metadata": {},
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looks like there is a recent solution for using pipleine with azure batch scoring\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-pipeline-batch-scoring-classification\n",
    "\n",
    "\n",
    "For more info, https://docs.microsoft.com/en-us/azure/machine-learning/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}