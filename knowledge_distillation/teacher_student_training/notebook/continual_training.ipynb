{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Continual training for DSAT queries\n",
    "\n",
    "In this notebook we will show how to resolve DSAT queries via continuial training on pre-trained student network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.85\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Workspace and select gpu cluster\n",
    "\n",
    "if there is not existing cluster, create one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library configuration succeeded\n",
      "https://ms.portal.azure.com/#@microsoft.onmicrosoft.com/resource/subscriptions/ddb33dc4-889c-4fa1-90ce-482d793d6480/resourceGroups/DevExp/providers/Microsoft.MachineLearningServices/workspaces/DevExperimentation\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "#subscription_id = \"4a66f470-dd54-4c5e-bd19-8cb65a426003\"\n",
    "#resource_group  = \"AML_Playground\"\n",
    "#workspace_name  = \"Teams_ws\"\n",
    "\n",
    "subscription_id = \"ddb33dc4-889c-4fa1-90ce-482d793d6480\"\n",
    "resource_group = \"DevExp\"\n",
    "workspace_name = \"DevExperimentation\"\n",
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "    ws.write_config()\n",
    "    print('Library configuration succeeded')\n",
    "    print('https://ms.portal.azure.com/#@microsoft.onmicrosoft.com/resource' + ws.get_details()['id'])\n",
    "except:\n",
    "    print('Workspace not found')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "#cluster_name = \"p100cluster\"\n",
    "cluster_name  =\"P100-SingleGPU\"\n",
    "\n",
    "try:\n",
    "    compute_target = ws.compute_targets[cluster_name]\n",
    "    print('Found existing compute target.')\n",
    "except KeyError:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC6s_v2', \n",
    "                                                           idle_seconds_before_scaledown=1800,\n",
    "                                                           min_nodes=0, \n",
    "                                                           max_nodes=10)\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Datastore and upload local data\n",
    "\n",
    "When you have large data and model, you need to create one seperate Datastore.\n",
    "\n",
    "If not, AML will have error and you can't track your outputs. \n",
    "\n",
    "Each workspace is associated with a default Azure Blob datastore named 'workspaceblobstore'. In this work, we use this default datastore to store our local data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = ws.get_default_datastore()\n",
    "from azureml.core import Workspace,Datastore \n",
    "\n",
    "ds = Datastore.get(ws, datastore_name='compliant_lu_haochu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an experiment\n",
    "Create an Experiment to track all the runs in your workspace. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'Communication_Lu_DSAT_fix' \n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit your Job\n",
    "The follow section creates one pytorch estimator, you can easily specify your parameters.\n",
    "\n",
    "When you submit your job, it will autoamtically upload your local repo to the cloud cluster. \n",
    "\n",
    "You can also submit tensorflow or keras job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Finetuning\n",
    "\n",
    "With pre-trained model and labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - framework_version is not specified, defaulting to version 1.3.\n"
     ]
    }
   ],
   "source": [
    "##BATCH AI\n",
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "\n",
    "\n",
    "script_params = {\n",
    "   \n",
    "    '--data_dir': ds.path(f'datasets/Communication_prod_data').as_mount(),     \n",
    "    #'--valid_dir':ds.path(f'datasets/Communication_prod_data/Mustpass_full.txt').as_mount(),\n",
    "    #'--train_dir':ds.path(f'datasets/Communication_prod_data/Mustpass_full.txt').as_mount(),\n",
    "    '--train_dir':'sample_data\\Mustpass.txt',\n",
    "    '--valid_dir':'sample_data\\Mustpass.txt',\n",
    "    \n",
    "    '--test_generated_dir':ds.path(f'datasets/Teams_communication/generated_data/communication_message_generated_no_contact.txt').as_mount(),\n",
    "    '--test_generated_no_contact_dir':ds.path(f'datasets/Teams_communication/generated_data/communication_message_generated_contact.txt').as_mount(),\n",
    "    '--target_set_dir':ds.path(f'datasets/Communication_prod_data/Mustpass_full.txt').as_mount(),\n",
    "    '--target_set_dir':ds.path(f'datasets/Communication_prod_data/Mustpass_full.txt').as_mount(),\n",
    "    #'--target_set_dir':ds.path(f'datasets/Teams_communication/Target_set_message_new_conll.txt').as_mount(),\n",
    "    \n",
    "    \n",
    "    ##for uncased longer teacher\n",
    "    '--student_model_dir': ds.path(f'Communication_student_model/MV4_model_cleanup').as_mount(),\n",
    "    '--teacher_model_path':ds.path(f'bert_data/uncased_model/outputs_base_uncased_no_basic_tokenizer').as_mount(),#from prod labeled sources\n",
    "    \n",
    "    '--bert_model':'bert-base-uncased', \n",
    "    '--do_lower_case':'',\n",
    "    \n",
    "    \n",
    "    \n",
    "    '--task_name':'ner',\n",
    "    '--output_dir':'./outputs',\n",
    "    \n",
    "    \n",
    "    \n",
    "    '--do_continual_training':'',\n",
    "    '--do_eval':'',\n",
    "    '--encoder_type':'GRU',\n",
    "    '--hidden_units':'300',\n",
    "    '--eval_batch_size':'1',\n",
    "\n",
    "    '--learning_rate':'1e-5',#smaller learning rate given init\n",
    "    '--num_train_epochs':'50', ##get larger number when you have smaller unsupervised data\n",
    "    '--warmup_proportion':'1',\n",
    "    #'--max_seq_length':'32',\n",
    "    '--max_seq_length':'128',\n",
    "    '--train_batch_size':'5',\n",
    "    \"--temperature\": '1',\n",
    "    \"--alpha\": '1',#for ablation study,weight for labeled data\n",
    "    \"--beta\": '1', #for ablation study,weight for unlabeled data\n",
    "    '--unsupervised_train_corpus':ds.path(f'datasets/Teams_communication/raw_generated_data/rawquery_merged.txt').as_mount()\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "estimator10 = PyTorch(source_directory='..', \n",
    "                    script_params=script_params,\n",
    "                    compute_target=compute_target, \n",
    "                    entry_script='src/Continual_training.py',\n",
    "                    #pip_packages=['pandas','pytorch-pretrained-bert==0.4.0','seqeval==0.0.5'],\n",
    "                    #pip_packages=['pandas','pytorch-pretrained-bert==0.6.1','seqeval==0.0.5','nltk'],\n",
    "                    pip_packages=['pandas','pytorch-pretrained-bert==0.6.1','seqeval==0.0.5','transformers==2.1.1','nltk'],\n",
    "                    use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up your running environment\n",
    "\n",
    "You can create your own virtual environment. It takes a while the first time you submit the job. If you do not change dependency, the job submission will be fast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcr.microsoft.com/azureml/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04\n"
     ]
    }
   ],
   "source": [
    "print(estimator10.run_config.environment.docker.base_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Conda environment specification. The dependencies defined in this file will\r\n",
      "# be automatically provisioned for runs with userManagedDependencies=False.\r\n",
      "\n",
      "# Details about the Conda environment file format:\r\n",
      "# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\r\n",
      "\n",
      "name: project_environment\n",
      "dependencies:\n",
      "  # The python interpreter version.\r\n",
      "  # Currently Azure ML only supports 3.5.2 and later.\r\n",
      "- python=3.6.2\n",
      "\n",
      "- pip:\n",
      "  - pandas\n",
      "  - pytorch-pretrained-bert==0.6.1\n",
      "  - seqeval==0.0.5\n",
      "  - transformers==2.1.1\n",
      "  - nltk\n",
      "  - azureml-defaults\n",
      "  - torch==1.3.1\n",
      "  - torchvision==0.4.1\n",
      "  - horovod==0.18.1\n",
      "  - tensorboard==1.14.0\n",
      "  - future==0.17.1\n",
      "channels:\n",
      "- conda-forge\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(estimator10.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit and Monitor your run\n",
    "\n",
    "You can also find your previous runs if your open the azure portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Validation identified the following error(s): \n",
      "..\\src\\bert_inference.py:321:14: F821 undefined name 'argparse'\r\n"
     ]
    }
   ],
   "source": [
    "run = experiment.submit(estimator10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allennlp",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
